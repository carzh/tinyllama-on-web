{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dc0eca-01a8-4640-9c05-33bb9ccfc292",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "818295ab-2b6a-4621-be90-4c1c7093122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath=\"models/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "modelpath=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "dataset_name=\"g-ronimo/oasst2_top1_en\"\n",
    "lr=0.00002      # learning rate\n",
    "bs=1            # batch size\n",
    "bs_eval=16      # batch size for evals\n",
    "ga_steps=16     # gradient acc. steps\n",
    "epochs=4\n",
    "# max_length=2048      # samples max. length\n",
    "max_length=32000      # samples max. length\n",
    "output_dir=\"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd920d4-6486-491f-afd7-f1c28d5cbb6c",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e867ab6-1b91-45a2-b08e-ec58cd433b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\geronimo-tinyllama-finetune\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False)    # fast tokenizer sometimes ignores added tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccbe5a-4ee1-4615-8e72-8d4103a315b7",
   "metadata": {},
   "source": [
    "# Add ChatML tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ece54e-49ef-4d8b-b957-8b2413eef555",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "    # pad_to_multiple_of=64)   # phi2 default is 64, see configuration_phi.py\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0f765-dd58-40d8-8648-abfe8d157c6c",
   "metadata": {},
   "source": [
    "# Load and prepare OA2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2dc07d-fddd-4c20-88ed-aeb5c4257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4877/4877 [01:36<00:00, 50.79 examples/s] \n",
      "Map: 100%|██████████| 542/542 [00:10<00:00, 49.74 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# chatML Template and tokenize dataset\n",
    "templates=[\n",
    "    \"<|im_start|>assistant\\n{msg}<|im_end|>\",\n",
    "    \"<|im_start|>user\\n{msg}<|im_end|>\"\n",
    "]\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "# tokenize dataset, set input_ids and attention_mask to train on assistant outputs only\n",
    "def tokenize(input, max_length):\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for i,msg in enumerate(input[\"conversation\"]):\n",
    "        isHuman = msg[\"role\"]==\"user\"\n",
    "        msg_chatml=templates[isHuman].format(msg=msg[\"content\"])\n",
    "        msg_tokenized=tokenizer(msg_chatml, truncation=False, add_special_tokens=False, max_length=max_length, padding=\"max_length\")\n",
    "    \n",
    "        input_ids+=msg_tokenized[\"input_ids\"]\n",
    "        attention_mask+=msg_tokenized[\"attention_mask\"]\n",
    "        labels+=[IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) if isHuman else msg_tokenized[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids[:max_length],\n",
    "        \"attention_mask\": attention_mask[:max_length],\n",
    "        \"labels\": labels[:max_length],\n",
    "    }\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    partial(tokenize, max_length=max_length), \n",
    "    batched=False, \n",
    "    # num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=dataset[\"train\"].column_names  # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d868b086-3863-40eb-a104-fb734bc355f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f26ede-411e-4685-81f4-368a5ecc6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2da59e2c-f190-4869-972a-9070c7e6de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "def collate(elements):\n",
    "    tokens=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokens])\n",
    "\n",
    "    for i,sample in enumerate(elements):\n",
    "        input_ids=sample[\"input_ids\"]\n",
    "        labels=sample[\"labels\"]\n",
    "        attention_mask=sample[\"attention_mask\"]\n",
    "\n",
    "        pad_len=tokens_maxlen-len(input_ids)\n",
    "\n",
    "        input_ids.extend( pad_len * [tokenizer.pad_token_id] )   \n",
    "        labels.extend( pad_len * [IGNORE_INDEX] )    \n",
    "        attention_mask.extend( pad_len * [0] ) \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor( [e[\"input_ids\"] for e in elements] ).numpy(),\n",
    "        \"labels\": torch.tensor( [e[\"labels\"] for e in elements] ).numpy(),\n",
    "        \"attention_mask\": torch.tensor( [e[\"attention_mask\"] for e in elements] ).numpy(),\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7357-6cdc-4355-9b7c-05aa518e214e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57edbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime.training.api as ort_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c894e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ort_api.CheckpointState.load_checkpoint('artifacts_generated_l1/checkpoint')\n",
    "training_model = ort_api.Module('artifacts_generated_l1/training_model.onnx', state, 'artifacts_generated_l1/eval_model.onnx')\n",
    "optimizer = ort_api.Optimizer('artifacts_generated_l1/optimizer_model.onnx', training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03c7c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8900b0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model.input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95514b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch():\n",
    "    training_model.train()\n",
    "    losses = []\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        print(i, 'out of', len(dataloader))\n",
    "        forward_inputs = [batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]]\n",
    "        print(\"input ids shape\", batch[\"input_ids\"].shape)\n",
    "        print(\"attention mask shape\", batch[\"attention_mask\"].shape)\n",
    "        print(\"labels shape\", batch[\"labels\"].shape)\n",
    "\n",
    "        loss, _ = training_model(*forward_inputs)\n",
    "        print('after training acll')\n",
    "        optimizer.step()\n",
    "        training_model.lazy_reset_grad()\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ee14fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 4877\n",
      "input ids shape (1, 32000)\n",
      "attention mask shape (1, 32000)\n",
      "labels shape (1, 32000)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:632 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/model/embed_tokens/Gather' Status Message: indices element out of data bounds, idx=32000 must be within the inclusive range [-32000,31999]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainEpoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrainEpoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention mask shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 12\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter training acll\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\geronimo-tinyllama-finetune\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:113\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *user_inputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(OrtValue(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m fetches)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_np_input(user_inputs):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_generic_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43muser_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_step_with_ortvalues(user_inputs)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\geronimo-tinyllama-finetune\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:85\u001b[0m, in \u001b[0;36mModule.__call__.<locals>._take_generic_step\u001b[1;34m(forward_inputs)\u001b[0m\n\u001b[0;32m     83\u001b[0m fetches \u001b[38;5;241m=\u001b[39m OrtValueVector()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval_step(forward_inputs, fetches)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:632 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/model/embed_tokens/Gather' Status Message: indices element out of data bounds, idx=32000 must be within the inclusive range [-32000,31999]\n"
     ]
    }
   ],
   "source": [
    "trainEpoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
