{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35dc354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\tinyllama-full\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from onnxruntime.training import artifacts\n",
    "import onnxruntime.training.api as ort_api\n",
    "import torch\n",
    "import onnx\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc0eca-01a8-4640-9c05-33bb9ccfc292",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818295ab-2b6a-4621-be90-4c1c7093122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath=\"models/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "modelpath=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "dataset_name=\"g-ronimo/oasst2_top1_en\"\n",
    "lr=0.00002      # learning rate\n",
    "bs=2            # batch size\n",
    "bs_eval=16      # batch size for evals\n",
    "ga_steps=16     # gradient acc. steps\n",
    "epochs=4\n",
    "max_length=2048      # samples max. length\n",
    "output_dir=\"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd920d4-6486-491f-afd7-f1c28d5cbb6c",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e867ab6-1b91-45a2-b08e-ec58cd433b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     modelpath,    \n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # attn_implementation=\"flash_attention_2\",\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False)    # fast tokenizer sometimes ignores added tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccbe5a-4ee1-4615-8e72-8d4103a315b7",
   "metadata": {},
   "source": [
    "# Add ChatML tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ece54e-49ef-4d8b-b957-8b2413eef555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0f765-dd58-40d8-8648-abfe8d157c6c",
   "metadata": {},
   "source": [
    "# Load and prepare OA2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2dc07d-fddd-4c20-88ed-aeb5c4257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  14%|█▍        | 671/4877 [00:01<00:11, 373.41 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2981 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 4877/4877 [00:13<00:00, 354.93 examples/s]\n",
      "Map: 100%|██████████| 542/542 [00:01<00:00, 346.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# chatML Template and tokenize dataset\n",
    "templates=[\n",
    "    \"<|im_start|>assistant\\n{msg}<|im_end|>\",\n",
    "    \"<|im_start|>user\\n{msg}<|im_end|>\"\n",
    "]\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "def get_position_ids(attention_mask):\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    # Shape: (batch_size, sequence_length)\n",
    "    return position_ids\n",
    "\n",
    "# tokenize dataset, set input_ids and attention_mask to train on assistant outputs only\n",
    "def tokenize(input, max_length):\n",
    "    input_ids, attention_mask, position_ids, labels = [], [], [], []\n",
    "\n",
    "    for i,msg in enumerate(input[\"conversation\"]):\n",
    "        isHuman = msg[\"role\"]==\"user\"\n",
    "        msg_chatml=templates[isHuman].format(msg=msg[\"content\"])\n",
    "        msg_tokenized=tokenizer(msg_chatml, truncation=False, add_special_tokens=False)\n",
    "\n",
    "        input_ids+=msg_tokenized[\"input_ids\"]\n",
    "        attention_mask+=msg_tokenized[\"attention_mask\"]\n",
    "        labels+=[IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) if isHuman else msg_tokenized[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids[:max_length],\n",
    "        \"attention_mask\": attention_mask[:max_length],\n",
    "        \"position_ids\": get_position_ids(torch.tensor(attention_mask[:max_length])),\n",
    "        \"labels\": labels[:max_length],\n",
    "    }\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    partial(tokenize, max_length=max_length), \n",
    "    batched=False, \n",
    "    # num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=dataset[\"train\"].column_names  # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868b086-3863-40eb-a104-fb734bc355f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f26ede-411e-4685-81f4-368a5ecc6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'position_ids', 'labels'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'position_ids', 'labels'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da59e2c-f190-4869-972a-9070c7e6de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "def collate(elements):\n",
    "    tokens=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokens])\n",
    "\n",
    "    for i,sample in enumerate(elements):\n",
    "        input_ids=sample[\"input_ids\"]\n",
    "        labels=sample[\"labels\"]\n",
    "        position_ids=sample[\"position_ids\"]\n",
    "        attention_mask=sample[\"attention_mask\"]\n",
    "\n",
    "        pad_len=tokens_maxlen-len(input_ids)\n",
    "\n",
    "        input_ids.extend( pad_len * [tokenizer.pad_token_id] )   \n",
    "        labels.extend( pad_len * [IGNORE_INDEX] )    \n",
    "        position_ids.extend( pad_len * [1] )\n",
    "        attention_mask.extend( pad_len * [0] ) \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor( [e[\"input_ids\"] for e in elements] ).numpy(),\n",
    "        \"labels\": torch.tensor( [e[\"labels\"] for e in elements] ).numpy(),\n",
    "        \"position_ids\": torch.tensor( [e[\"position_ids\"] for e in elements] ).numpy(),\n",
    "        # \"position_ids\": position_ids.numpy(),\n",
    "        \"attention_mask\": torch.tensor( [e[\"attention_mask\"] for e in elements] ).numpy(),\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54b7cb",
   "metadata": {},
   "source": [
    "# Generating artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids (2, 530) 32002\n",
      "labels (2, 530) 32002\n",
      "position_ids (2, 530) 529\n",
      "attention_mask (2, 530) 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# transformers_model = transformers.LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", ignore_mismatched_sizes=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True, collate_fn = collate)\n",
    "\n",
    "batch = {}\n",
    "for batch_from_dl in dataloader:\n",
    "    batch = batch_from_dl\n",
    "    break\n",
    "\n",
    "# inputs = (torch.tensor(batch['input_ids'], dtype=torch.int64), torch.tensor(batch['attention_mask'], dtype=torch.int64))\n",
    "# print(inputs[0])\n",
    "# print(inputs[1].shape)\n",
    "\n",
    "for x in batch.keys():\n",
    "    print(x, batch[x].shape, batch[x].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f252c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"attention_mask\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"position_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"labels\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "onnx_model_path = \"rank_0_TinyLlama-1.1B-Chat-v1.0_decoder_merged_model_fp32.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path, load_external_data=False)\n",
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27436b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"rank_0_TinyLlama-1.1B-Chat-v1.0_decoder_merged_model_fp32.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path, load_external_data=False)\n",
    "requires_grad = [param.name for param in onnx_model.graph.initializer] # if param.name not in requires_grad]\n",
    "frozen_params = []\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    # loss=artifacts.LossType.CrossEntropyLoss,\n",
    "    artifact_directory=\"artifacts_generated_full_test\",\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    ort_format=False,\n",
    "    # loss_input_names=[\"loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f435b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': name: \"loss\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", 'logits': name: \"logits\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence_length\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 32000\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "name_graph_output_mapping = {output.name: output for output in onnx_model.graph.output}\n",
    "print(name_graph_output_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7357-6cdc-4355-9b7c-05aa518e214e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c894e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = ort_api.CheckpointState.load_checkpoint('artifacts_generated_l1/checkpoint')\n",
    "# training_model = ort_api.Module('artifacts_generated_l1/training_model_corrected_labels.onnx', state, 'artifacts_generated_l1/eval_model.onnx')\n",
    "# optimizer = ort_api.Optimizer('artifacts_generated_l1/optimizer_model.onnx', training_model)\n",
    "\n",
    "state = ort_api.CheckpointState.load_checkpoint('artifacts_generated_full_test/checkpoint')\n",
    "training_model = ort_api.Module('artifacts_generated_full_test/training_model.onnx', state, 'artifacts_generated_full_test/eval_model.onnx')\n",
    "optimizer = ort_api.Optimizer('artifacts_generated_full_test/optimizer_model.onnx', training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3e629ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"attention_mask\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"position_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"labels\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch_size\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence_length\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.embed_tokens.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.q_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.k_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.v_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.o_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.gate_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.up_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.down_proj.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.input_layernorm.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.post_attention_layernorm.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.norm.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"lm_head.weight\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.embed_tokens.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.q_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.k_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.v_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 256\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.self_attn.o_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.gate_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.up_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.mlp.down_proj.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 5632\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.input_layernorm.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.layers.0.post_attention_layernorm.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"model.norm.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"lm_head.weight_grad.accumulation.buffer\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 2048\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"lazy_reset_grad\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 9\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_onnx = onnx.load('artifacts_generated_full_test/training_model.onnx')\n",
    "training_onnx.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a4b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"model.embed_tokens.weight\"\n",
      "input: \"input_ids\"\n",
      "output: \"/model/embed_tokens/Gather_output_0\"\n",
      "name: \"/model/embed_tokens/Gather\"\n",
      "op_type: \"Gather\"\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  type: INT\n",
      "  i: 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in training_onnx.graph.node:\n",
    "    if node.name == \"/model/embed_tokens/Gather\":\n",
    "        print(node) # or save it to another variable to access its elements\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c7c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc0710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids (2, 518) 32002\n",
      "labels (2, 518) 32002\n",
      "position_ids (2, 518) 517\n",
      "attention_mask (2, 518) 1\n"
     ]
    }
   ],
   "source": [
    "batch = {}\n",
    "for batch_from_dl in dataloader:\n",
    "    batch = batch_from_dl\n",
    "    break\n",
    "\n",
    "# inputs = (torch.tensor(batch['input_ids'], dtype=torch.int64), torch.tensor(batch['attention_mask'], dtype=torch.int64))\n",
    "# print(inputs[0])\n",
    "# print(inputs[1].shape)\n",
    "\n",
    "for x in batch.keys():\n",
    "    print(x, batch[x].shape, batch[x].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8900b0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'position_ids', 'labels']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model.input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95514b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch():\n",
    "    training_model.train()\n",
    "    losses = []\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        print(i, 'out of', len(dataloader))\n",
    "        forward_inputs = [batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"position_ids\"], batch[\"labels\"]]\n",
    "        # print(batch.keys())\n",
    "        # print(\"input ids shape\", batch[\"input_ids\"].shape)\n",
    "        # print(\"attention mask shape\", batch[\"attention_mask\"].shape)\n",
    "        # print(\"position_ids shape\", batch[\"position_ids\"].shape)\n",
    "        # print(\"labels shape\", batch[\"labels\"].shape)\n",
    "\n",
    "        loss, _ = training_model(*forward_inputs)\n",
    "        # print('after training acll')\n",
    "        optimizer.step()\n",
    "        training_model.lazy_reset_grad()\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee14fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 2439\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "C:\\Users\\carolinezhu\\Documents\\onnxruntime\\orttraining\\orttraining\\training_api\\module.cc:632 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/embed_tokens/Gather' Status Message: indices element out of data bounds, idx=32000 must be within the inclusive range [-32000,31999]\nStacktrace:\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\orttraining\\orttraining\\python\\orttraining_pybind_state.cc(691): onnxruntime_pybind11_state!<lambda_6c99460bc5fb44db08ea39edf4dac239>::operator()+0x4F4\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\cast.h(1440): onnxruntime_pybind11_state!pybind11::detail::argument_loader<onnxruntime::training::api::Module *,std::vector<pybind11::object,std::allocator<pybind11::object> > const &,std::vector<OrtValue,std::allocator<OrtValue> > &>::call_impl<void,<lambda_6c99460bc5fb44db08ea39edf4dac239> &,0,1,2,pybind11::detail::void_type>+0xA7\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\cast.h(1415): onnxruntime_pybind11_state!pybind11::detail::argument_loader<onnxruntime::training::api::Module *,std::vector<pybind11::object,std::allocator<pybind11::object> > const &,std::vector<OrtValue,std::allocator<OrtValue> > &>::call<void,pybind11::detail::void_type,<lambda_6c99460bc5fb44db08ea39edf4dac239> &>+0x80\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(249): onnxruntime_pybind11_state!<lambda_ef5e7e3c8b97e912a8fb54ee33c997d1>::operator()+0x171\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(167): onnxruntime_pybind11_state!<lambda_ef5e7e3c8b97e912a8fb54ee33c997d1>::<lambda_invoker_cdecl>+0x20\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(929): onnxruntime_pybind11_state!pybind11::cpp_function::dispatcher+0x13AD\n(0): python39!PyArg_ParseTuple_SizeT+0x1D6A\n(0): python39!PyObject_MakeTpCall+0xE9\n(0): python39!PyErr_FormatFromCauseTstate+0xDD4F\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x8BB\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!Py_NewReference+0x92C\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyFunction_Vectorcall+0x23D\n(0): python39!PyObject_FastCallDictTstate+0x63\n(0): python39!PyObject_Call_Prepend+0x7B\n(0): python39!PyWeakref_NewProxy+0x22A8\n(0): python39!PyObject_Call+0x1A0\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyEval_EvalCodeWithName+0xA9\n(0): python39!PyEval_EvalCodeEx+0x9B\n(0): python39!PyEval_EvalCode+0x2D\n(0): python39!PyFuture_FromASTObject+0x46A\n(0): python39!PyFuture_FromASTObject+0x373\n(0): python39!Py_NewReference+0x850\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyBytesWriter_Finish+0x251\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!PyVectorcall_Call+0xB8\n(0): python39!PyObject_Call+0x13E\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0xE28\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): _asyncio!PyInit__asyncio+0x55C5\n(0): _asyncio!PyInit__asyncio+0x53FB\n(0): _asyncio!PyInit__asyncio+0x5CA7\n(0): _asyncio!PyInit__asyncio+0x306A\n(0): python39!PyObject_MakeTpCall+0xE9\n(0): python39!PyContext_NewHamtForTests+0x4A\n(0): python39!PyContext_NewHamtForTests+0x3C9\n(0): python39!PyArg_CheckPositional+0x12E\n(0): python39!PyVectorcall_Call+0x5C\n(0): python39!PyObject_Call+0x4F\n(0): python39!PyObject_Call+0x174\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x8BB\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyEval_EvalCodeWithName+0xA9\n(0): python39!PyEval_EvalCodeEx+0x9B\n(0): python39!PyEval_EvalCode+0x2D\n(0): python39!PyFuture_FromASTObject+0x46A\n(0): python39!PyFuture_FromASTObject+0x373\n(0): python39!Py_NewReference+0x850\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!Py_NewReference+0x92C\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyFunction_Vectorcall+0x23D\n(0): python39!PyVectorcall_Call+0x5C\n(0): python39!PyObject_Call+0x4F\n(0): python39!Py_MakePendingCalls+0x4FE\n(0): python39!Py_RunMain+0x143\n(0): python39!Py_RunMain+0x15\n(0): python39!Py_Main+0x6F\n(0): python39!Py_Main+0x25\n(0): python+0x1268\n(0): KERNEL32!BaseThreadInitThunk+0x1D\n(0): ntdll!RtlUserThreadStart+0x28\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainEpoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mtrainEpoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m forward_inputs \u001b[38;5;241m=\u001b[39m [batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(batch.keys())\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"input ids shape\", batch[\"input_ids\"].shape)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(\"attention mask shape\", batch[\"attention_mask\"].shape)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(\"position_ids shape\", batch[\"position_ids\"].shape)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(\"labels shape\", batch[\"labels\"].shape)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print('after training acll')\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\tinyllama-full\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:113\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *user_inputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(OrtValue(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m fetches)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_np_input(user_inputs):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_generic_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43muser_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_step_with_ortvalues(user_inputs)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\tinyllama-full\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:85\u001b[0m, in \u001b[0;36mModule.__call__.<locals>._take_generic_step\u001b[1;34m(forward_inputs)\u001b[0m\n\u001b[0;32m     83\u001b[0m fetches \u001b[38;5;241m=\u001b[39m OrtValueVector()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval_step(forward_inputs, fetches)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: C:\\Users\\carolinezhu\\Documents\\onnxruntime\\orttraining\\orttraining\\training_api\\module.cc:632 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/embed_tokens/Gather' Status Message: indices element out of data bounds, idx=32000 must be within the inclusive range [-32000,31999]\nStacktrace:\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\orttraining\\orttraining\\python\\orttraining_pybind_state.cc(691): onnxruntime_pybind11_state!<lambda_6c99460bc5fb44db08ea39edf4dac239>::operator()+0x4F4\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\cast.h(1440): onnxruntime_pybind11_state!pybind11::detail::argument_loader<onnxruntime::training::api::Module *,std::vector<pybind11::object,std::allocator<pybind11::object> > const &,std::vector<OrtValue,std::allocator<OrtValue> > &>::call_impl<void,<lambda_6c99460bc5fb44db08ea39edf4dac239> &,0,1,2,pybind11::detail::void_type>+0xA7\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\cast.h(1415): onnxruntime_pybind11_state!pybind11::detail::argument_loader<onnxruntime::training::api::Module *,std::vector<pybind11::object,std::allocator<pybind11::object> > const &,std::vector<OrtValue,std::allocator<OrtValue> > &>::call<void,pybind11::detail::void_type,<lambda_6c99460bc5fb44db08ea39edf4dac239> &>+0x80\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(249): onnxruntime_pybind11_state!<lambda_ef5e7e3c8b97e912a8fb54ee33c997d1>::operator()+0x171\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(167): onnxruntime_pybind11_state!<lambda_ef5e7e3c8b97e912a8fb54ee33c997d1>::<lambda_invoker_cdecl>+0x20\nC:\\Users\\carolinezhu\\Documents\\onnxruntime\\build\\Windows\\Debug\\_deps\\pybind11_project-src\\include\\pybind11\\pybind11.h(929): onnxruntime_pybind11_state!pybind11::cpp_function::dispatcher+0x13AD\n(0): python39!PyArg_ParseTuple_SizeT+0x1D6A\n(0): python39!PyObject_MakeTpCall+0xE9\n(0): python39!PyErr_FormatFromCauseTstate+0xDD4F\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x8BB\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!Py_NewReference+0x92C\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyFunction_Vectorcall+0x23D\n(0): python39!PyObject_FastCallDictTstate+0x63\n(0): python39!PyObject_Call_Prepend+0x7B\n(0): python39!PyWeakref_NewProxy+0x22A8\n(0): python39!PyObject_Call+0x1A0\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyEval_EvalCodeWithName+0xA9\n(0): python39!PyEval_EvalCodeEx+0x9B\n(0): python39!PyEval_EvalCode+0x2D\n(0): python39!PyFuture_FromASTObject+0x46A\n(0): python39!PyFuture_FromASTObject+0x373\n(0): python39!Py_NewReference+0x850\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyBytesWriter_Finish+0x251\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!PyVectorcall_Call+0xB8\n(0): python39!PyObject_Call+0x13E\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0xE28\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): python39!PyEval_EvalFrameDefault+0x1E9A\n(0): python39!PyArg_CheckPositional+0x2116\n(0): _asyncio!PyInit__asyncio+0x55C5\n(0): _asyncio!PyInit__asyncio+0x53FB\n(0): _asyncio!PyInit__asyncio+0x5CA7\n(0): _asyncio!PyInit__asyncio+0x306A\n(0): python39!PyObject_MakeTpCall+0xE9\n(0): python39!PyContext_NewHamtForTests+0x4A\n(0): python39!PyContext_NewHamtForTests+0x3C9\n(0): python39!PyArg_CheckPositional+0x12E\n(0): python39!PyVectorcall_Call+0x5C\n(0): python39!PyObject_Call+0x4F\n(0): python39!PyObject_Call+0x174\n(0): python39!PyEval_EvalFrameDefault+0x167E\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!Py_NewReference+0x546\n(0): python39!PyEval_EvalFrameDefault+0x693\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyType_GenericAlloc+0xB4A\n(0): python39!Py_NewReference+0x2C4\n(0): python39!PyEval_EvalFrameDefault+0x8BB\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyEval_EvalCodeWithName+0xA9\n(0): python39!PyEval_EvalCodeEx+0x9B\n(0): python39!PyEval_EvalCode+0x2D\n(0): python39!PyFuture_FromASTObject+0x46A\n(0): python39!PyFuture_FromASTObject+0x373\n(0): python39!Py_NewReference+0x850\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!Py_NewReference+0x92C\n(0): python39!PyEval_EvalFrameDefault+0x492\n(0): python39!PyFunction_Vectorcall+0x946\n(0): python39!PyFunction_Vectorcall+0x23D\n(0): python39!PyVectorcall_Call+0x5C\n(0): python39!PyObject_Call+0x4F\n(0): python39!Py_MakePendingCalls+0x4FE\n(0): python39!Py_RunMain+0x143\n(0): python39!Py_RunMain+0x15\n(0): python39!Py_Main+0x6F\n(0): python39!Py_Main+0x25\n(0): python+0x1268\n(0): KERNEL32!BaseThreadInitThunk+0x1D\n(0): ntdll!RtlUserThreadStart+0x28\n"
     ]
    }
   ],
   "source": [
    "trainEpoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f12a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"artifacts_generated_l1/training_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78053f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"labels\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 7\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"Castloss_dim_0\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 32000\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "dim {\n",
      "  dim_param: \"batch_size\"\n",
      "}\n",
      "dim {\n",
      "  dim_param: \"sequence_length\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.graph.input[2])\n",
    "import copy\n",
    "labels_input = copy.deepcopy(model.graph.input[0])\n",
    "labels_input.name = \"labels\"\n",
    "labels_input.type.tensor_type.elem_type = onnx.TensorProto.INT64\n",
    "model.graph.input[2].CopyFrom(labels_input)\n",
    "print(model.graph.input[2].type.tensor_type.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0775ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model, \"artifacts_generated_l1/training_model_corrected_labels.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
