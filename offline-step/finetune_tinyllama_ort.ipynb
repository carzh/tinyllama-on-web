{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c35dc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from onnxruntime.training import artifacts\n",
    "import onnxruntime.training.api as ort_api\n",
    "import torch\n",
    "import onnx\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc0eca-01a8-4640-9c05-33bb9ccfc292",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818295ab-2b6a-4621-be90-4c1c7093122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath=\"models/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "modelpath=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "dataset_name=\"g-ronimo/oasst2_top1_en\"\n",
    "lr=0.00002      # learning rate\n",
    "bs=1            # batch size\n",
    "bs_eval=16      # batch size for evals\n",
    "ga_steps=16     # gradient acc. steps\n",
    "epochs=4\n",
    "max_length=2048      # samples max. length\n",
    "output_dir=\"out\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd920d4-6486-491f-afd7-f1c28d5cbb6c",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e867ab6-1b91-45a2-b08e-ec58cd433b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     modelpath,    \n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # attn_implementation=\"flash_attention_2\",\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False)    # fast tokenizer sometimes ignores added tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccbe5a-4ee1-4615-8e72-8d4103a315b7",
   "metadata": {},
   "source": [
    "# Add ChatML tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ece54e-49ef-4d8b-b957-8b2413eef555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0f765-dd58-40d8-8648-abfe8d157c6c",
   "metadata": {},
   "source": [
    "# Load and prepare OA2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb2dc07d-fddd-4c20-88ed-aeb5c4257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  11%|█         | 517/4877 [00:00<00:05, 801.74 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 4877/4877 [00:07<00:00, 672.67 examples/s]\n",
      "Map: 100%|██████████| 542/542 [00:00<00:00, 716.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# chatML Template and tokenize dataset\n",
    "templates=[\n",
    "    \"<|im_start|>assistant\\n{msg}<|im_end|>\",\n",
    "    \"<|im_start|>user\\n{msg}<|im_end|>\"\n",
    "]\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "# tokenize dataset, set input_ids and attention_mask to train on assistant outputs only\n",
    "def tokenize(input, max_length):\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    for i,msg in enumerate(input[\"conversation\"]):\n",
    "        isHuman = msg[\"role\"]==\"user\"\n",
    "        msg_chatml=templates[isHuman].format(msg=msg[\"content\"])\n",
    "        msg_tokenized=tokenizer(msg_chatml, truncation=False, add_special_tokens=False)\n",
    "    \n",
    "        input_ids+=msg_tokenized[\"input_ids\"]\n",
    "        attention_mask+=msg_tokenized[\"attention_mask\"]\n",
    "        labels+=[IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) if isHuman else msg_tokenized[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids[:max_length],\n",
    "        \"attention_mask\": attention_mask[:max_length],\n",
    "        \"labels\": labels[:max_length],\n",
    "    }\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    partial(tokenize, max_length=max_length), \n",
    "    batched=False, \n",
    "    # num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=dataset[\"train\"].column_names  # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d868b086-3863-40eb-a104-fb734bc355f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversation'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f26ede-411e-4685-81f4-368a5ecc6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4877\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da59e2c-f190-4869-972a-9070c7e6de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "def collate(elements):\n",
    "    tokens=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokens])\n",
    "\n",
    "    for i,sample in enumerate(elements):\n",
    "        input_ids=sample[\"input_ids\"]\n",
    "        labels=sample[\"labels\"]\n",
    "        attention_mask=sample[\"attention_mask\"]\n",
    "\n",
    "        pad_len=tokens_maxlen-len(input_ids)\n",
    "\n",
    "        input_ids.extend( pad_len * [tokenizer.pad_token_id] )   \n",
    "        labels.extend( pad_len * [IGNORE_INDEX] )    \n",
    "        attention_mask.extend( pad_len * [0] ) \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor( [e[\"input_ids\"] for e in elements] ).numpy(),\n",
    "        \"labels\": torch.tensor( [e[\"labels\"] for e in elements] ).numpy(),\n",
    "        \"attention_mask\": torch.tensor( [e[\"attention_mask\"] for e in elements] ).numpy(),\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54b7cb",
   "metadata": {},
   "source": [
    "# Generating artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22da2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 were not used when initializing LlamaForCausalLM: ['model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 202])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1057: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_length > self.causal_mask.shape[-1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with exporting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformers_model = transformers.LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", ignore_mismatched_sizes=True)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True, collate_fn = collate)\n",
    "\n",
    "batch = {}\n",
    "for batch_from_dl in dataloader:\n",
    "    batch = batch_from_dl\n",
    "    break\n",
    "\n",
    "inputs = (torch.tensor(batch['input_ids'], dtype=torch.int64), torch.tensor(batch['attention_mask'], dtype=torch.int64))\n",
    "print(inputs[0].shape)\n",
    "\n",
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs[0], inputs[1])\n",
    "\n",
    "model = FlatModel(transformers_model)\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  inputs,\n",
    "                  \"tinyllama_1_hidden.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  opset_version=14,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING,\n",
    "                  do_constant_folding=False,\n",
    "                  dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  }\n",
    "                  )\n",
    "\n",
    "print('done with exporting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7223235",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"tinyllama_1_hidden.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c27e436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"logits\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence_length\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"Addlogits_dim_2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(onnx_model.graph.output[1])\n",
    "del onnx_model.graph.output[1].type.tensor_type.shape.dim[3]\n",
    "print(onnx_model.graph.output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e13f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(onnx_model, \"tinyllama_1_hidden.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c27436b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceError",
     "evalue": "[ShapeInferenceError] (op_type:Add, node name: /model/model/layers.0/self_attn/Add_1): [ShapeInferenceError] Inferred shape and existing shape differ in rank: (4) vs (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m requires_grad \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m onnx_model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minitializer] \u001b[38;5;66;03m# if param.name not in requires_grad]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m frozen_params \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozen_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLossType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts_generated_full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mort_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_input_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:174\u001b[0m, in \u001b[0;36mgenerate_artifacts\u001b[1;34m(model, requires_grad, frozen_params, loss, optimizer, artifact_directory, prefix, ort_format, custom_op_library, additional_output_names, nominal_checkpoint, loss_input_names)\u001b[0m\n\u001b[0;32m    167\u001b[0m     custom_op_library_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(custom_op_library)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m onnxblock\u001b[38;5;241m.\u001b[39mbase(model), (\n\u001b[0;32m    170\u001b[0m     onnxblock\u001b[38;5;241m.\u001b[39mcustom_op_library(custom_op_library_path)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m custom_op_library \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    173\u001b[0m ):\n\u001b[1;32m--> 174\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     training_model, eval_model \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mto_model_proto()\n\u001b[0;32m    176\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mparameters()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\onnxblock.py:188\u001b[0m, in \u001b[0;36mTrainingBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    186\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding training block \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mshape_inference\u001b[38;5;241m.\u001b[39minfer_shapes(accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    192\u001b[0m _graph_utils\u001b[38;5;241m.\u001b[39mregister_graph_outputs(model, output)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:142\u001b[0m, in \u001b[0;36mgenerate_artifacts.<locals>._TrainingBlock.build\u001b[1;34m(self, *inputs_to_loss)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss_output, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(additional_output_names))\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_to_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\blocks.py:50\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding block: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 50\u001b[0m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnx\\checker.py:148\u001b[0m, in \u001b[0;36mcheck_model\u001b[1;34m(model, full_check, skip_opset_compatibility_check)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetsizeof(protobuf_string) \u001b[38;5;241m>\u001b[39m MAXIMUM_PROTOBUF:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis protobuf of onnx model is too large (>2GB). Call check_model with model path instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[1;32m--> 148\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotobuf_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_opset_compatibility_check\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInferenceError\u001b[0m: [ShapeInferenceError] (op_type:Add, node name: /model/model/layers.0/self_attn/Add_1): [ShapeInferenceError] Inferred shape and existing shape differ in rank: (4) vs (3)"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnx_model_path)\n",
    "requires_grad = [param.name for param in onnx_model.graph.initializer] # if param.name not in requires_grad]\n",
    "frozen_params = []\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss,\n",
    "    artifact_directory=\"artifacts_generated_full\",\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    ort_format=False,\n",
    "    loss_input_names=[\"loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7357-6cdc-4355-9b7c-05aa518e214e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c894e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = ort_api.CheckpointState.load_checkpoint('artifacts_generated_l1/checkpoint')\n",
    "# training_model = ort_api.Module('artifacts_generated_l1/training_model_corrected_labels.onnx', state, 'artifacts_generated_l1/eval_model.onnx')\n",
    "# optimizer = ort_api.Optimizer('artifacts_generated_l1/optimizer_model.onnx', training_model)\n",
    "\n",
    "state = ort_api.CheckpointState.load_checkpoint('artifacts_generated_full/checkpoint')\n",
    "training_model = ort_api.Module('artifacts_generated_full/training_model.onnx', state, 'artifacts_generated_full/eval_model.onnx')\n",
    "optimizer = ort_api.Optimizer('artifacts_generated_full/optimizer_model.onnx', training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03c7c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_tokenized[\"train\"], batch_size=bs, shuffle=True, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8900b0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model.input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f95514b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch():\n",
    "    training_model.train()\n",
    "    losses = []\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        print(i, 'out of', len(dataloader))\n",
    "        forward_inputs = [batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]]\n",
    "        print(\"input ids shape\", batch[\"input_ids\"].shape)\n",
    "        print(\"attention mask shape\", batch[\"attention_mask\"].shape)\n",
    "        print(\"labels shape\", batch[\"labels\"].shape)\n",
    "\n",
    "        loss, _ = training_model(*forward_inputs)\n",
    "        print('after training acll')\n",
    "        optimizer.step()\n",
    "        training_model.lazy_reset_grad()\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ee14fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 4877\n",
      "input ids shape (1, 964)\n",
      "attention mask shape (1, 964)\n",
      "labels shape (1, 964)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:538 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running SoftmaxCrossEntropyLoss node. Name:'onnx::SoftmaxCrossEntropyLoss::16' Status Message: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_ops\\cpu\\loss\\softmax_cross_entropy_loss.cc:69 onnxruntime::contrib::VerifyLogitWeightAndLabelShape label_shape[i + 1] == logit_shape[i + 2] was false. The shape of logit and label does not match\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainEpoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 12\u001b[0m, in \u001b[0;36mtrainEpoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention mask shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 12\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter training acll\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:113\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *user_inputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(OrtValue(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m fetches)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_np_input(user_inputs):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_generic_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43muser_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_step_with_ortvalues(user_inputs)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:85\u001b[0m, in \u001b[0;36mModule.__call__.<locals>._take_generic_step\u001b[1;34m(forward_inputs)\u001b[0m\n\u001b[0;32m     83\u001b[0m fetches \u001b[38;5;241m=\u001b[39m OrtValueVector()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval_step(forward_inputs, fetches)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:538 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running SoftmaxCrossEntropyLoss node. Name:'onnx::SoftmaxCrossEntropyLoss::16' Status Message: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_ops\\cpu\\loss\\softmax_cross_entropy_loss.cc:69 onnxruntime::contrib::VerifyLogitWeightAndLabelShape label_shape[i + 1] == logit_shape[i + 2] was false. The shape of logit and label does not match\n\n"
     ]
    }
   ],
   "source": [
    "trainEpoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f12a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"artifacts_generated_l1/training_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78053f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"labels\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 7\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"Castloss_dim_0\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 32000\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "dim {\n",
      "  dim_param: \"batch_size\"\n",
      "}\n",
      "dim {\n",
      "  dim_param: \"sequence_length\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.graph.input[2])\n",
    "import copy\n",
    "labels_input = copy.deepcopy(model.graph.input[0])\n",
    "labels_input.name = \"labels\"\n",
    "labels_input.type.tensor_type.elem_type = onnx.TensorProto.INT64\n",
    "model.graph.input[2].CopyFrom(labels_input)\n",
    "print(model.graph.input[2].type.tensor_type.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0775ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model, \"artifacts_generated_l1/training_model_corrected_labels.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
