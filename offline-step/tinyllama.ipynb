{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import torch\n",
    "import onnx\n",
    "import transformers\n",
    "# from fastT5 import export_and_get_onnx_model\n",
    "import onnxruntime.training.api as ort_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = export_and_get_onnx_model(\"MBZUAI/LaMini-T5-61M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"distilbert/distilgpt2\",\n",
    "# )\n",
    "\n",
    "# transformers_model = transformers.AutoModelForCausalLM.from_pretrained(\"MBZUAI/LaMini-Cerebras-111M\")\n",
    "# transformers_model = transformers.AutoModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "transformers_model = transformers.AutoModel.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# transformers_model_gpt = transformers.GPT2Model.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 464, 3139,  286, 4881,  318, 6342,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "# tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "# model = FlatModel(pipeline.model)\n",
    "model = FlatModel(transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 464, 3139,  286, 4881,  318, 6342,   13]])\n",
      "past key values: None\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:804: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n"
     ]
    }
   ],
   "source": [
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "                  \"distilgpt2.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING,\n",
    "                  do_constant_folding=False,\n",
    "                  opset_version=15,\n",
    "                  dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args ('loss', 'logits', 'value.3', 'key.11', 'value.11', 'key.19', 'value.19', 'key.27', 'value.27', 'key.35', 'value.35', 'key.43', 'value.43')\n",
      "args after edit ['loss', 'logits']\n"
     ]
    },
    {
     "ename": "InferenceError",
     "evalue": "[ShapeInferenceError] (op_type:SoftmaxCrossEntropyLoss, node name: onnx::SoftmaxCrossEntropyLoss::10): labels typestr: Tind, has unsupported type: tensor(float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# onnx_model = onnx.load(\"tinyllama.onnx\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozen_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLossType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:154\u001b[0m, in \u001b[0;36mgenerate_artifacts\u001b[1;34m(model, requires_grad, frozen_params, loss, optimizer, artifact_directory, **extra_options)\u001b[0m\n\u001b[0;32m    149\u001b[0m     custom_op_library \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(custom_op_library)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m onnxblock\u001b[38;5;241m.\u001b[39mbase(model), onnxblock\u001b[38;5;241m.\u001b[39mcustom_op_library(\n\u001b[0;32m    152\u001b[0m     custom_op_library\n\u001b[0;32m    153\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m custom_op_library \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[1;32m--> 154\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     training_model, eval_model \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mto_model_proto()\n\u001b[0;32m    156\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mparameters()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\onnxblock.py:191\u001b[0m, in \u001b[0;36mTrainingBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m args \u001b[38;5;241m=\u001b[39m [args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs after edit\u001b[39m\u001b[38;5;124m\"\u001b[39m, args)\n\u001b[1;32m--> 191\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mshape_inference\u001b[38;5;241m.\u001b[39minfer_shapes(accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    195\u001b[0m _graph_utils\u001b[38;5;241m.\u001b[39mregister_graph_outputs(model, output)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:124\u001b[0m, in \u001b[0;36mgenerate_artifacts.<locals>._TrainingBlock.build\u001b[1;34m(self, *inputs_to_loss)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss_output, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(extra_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_output_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_to_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\blocks.py:50\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding block: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 50\u001b[0m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnx\\checker.py:148\u001b[0m, in \u001b[0;36mcheck_model\u001b[1;34m(model, full_check, skip_opset_compatibility_check)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetsizeof(protobuf_string) \u001b[38;5;241m>\u001b[39m MAXIMUM_PROTOBUF:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis protobuf of onnx model is too large (>2GB). Call check_model with model path instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[1;32m--> 148\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotobuf_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_opset_compatibility_check\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInferenceError\u001b[0m: [ShapeInferenceError] (op_type:SoftmaxCrossEntropyLoss, node name: onnx::SoftmaxCrossEntropyLoss::10): labels typestr: Tind, has unsupported type: tensor(float)"
     ]
    }
   ],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "num_named_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_named_params += 1\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "# onnx_model = onnx.load(\"tinyllama.onnx\")\n",
    "onnx_model = onnx.load(\"distilgpt2.onnx\")\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    # loss=artifacts.LossType.CrossEntropyLoss,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test generated artifacts\n",
    "# create checkpoint state\n",
    "\n",
    "state = ort_api.CheckpointState.load_checkpoint(\"checkpoint\")\n",
    "\n",
    "model = ort_api.Module('training_model.onnx', state, 'eval_model.onnx')\n",
    "\n",
    "optimizer = ort_api.Optimizer('optimizer_model.onnx', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dialogueText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Hello folks, please help me a bit with the fol...\n",
      "1          Did I choose a bad channel? I ask because you ...\n",
      "2          the second sentence is better english   and we...\n",
      "3                                               Sock Puppe?t\n",
      "4                                                       WTF?\n",
      "                                 ...                        \n",
      "1038319                                           anyone on?\n",
      "1038320                                                  yes\n",
      "1038321    can I get a pastebin of someones menu.lst with...\n",
      "1038322                         http://pastebin.com/fe921690\n",
      "1038323                                               thanks\n",
      "Name: text, Length: 1038324, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
