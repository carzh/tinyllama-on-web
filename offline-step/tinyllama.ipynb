{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import torch\n",
    "import onnx\n",
    "import transformers\n",
    "# from fastT5 import export_and_get_onnx_model\n",
    "import onnxruntime.training.api as ort_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = export_and_get_onnx_model(\"MBZUAI/LaMini-T5-61M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"distilbert/distilgpt2\",\n",
    "# )\n",
    "\n",
    "# transformers_model = transformers.AutoModelForCausalLM.from_pretrained(\"MBZUAI/LaMini-Cerebras-111M\")\n",
    "# transformers_model = transformers.AutoModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "transformers_model = transformers.AutoModel.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# transformers_model_gpt = transformers.GPT2Model.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 464, 3139,  286, 4881,  318, 6342,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "# tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "# model = FlatModel(pipeline.model)\n",
    "model = FlatModel(transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 464, 3139,  286, 4881,  318, 6342,   13]])\n",
      "past key values: None\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:804: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n"
     ]
    }
   ],
   "source": [
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "                  \"distilgpt2.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING,\n",
    "                  do_constant_folding=False,\n",
    "                  opset_version=15,\n",
    "                  dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args ('loss', 'logits', 'value.3', 'key.11', 'value.11', 'key.19', 'value.19', 'key.27', 'value.27', 'key.35', 'value.35', 'key.43', 'value.43')\n",
      "args after edit ['loss', 'logits']\n"
     ]
    },
    {
     "ename": "InferenceError",
     "evalue": "[ShapeInferenceError] (op_type:SoftmaxCrossEntropyLoss, node name: onnx::SoftmaxCrossEntropyLoss::10): labels typestr: Tind, has unsupported type: tensor(float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# onnx_model = onnx.load(\"tinyllama.onnx\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozen_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLossType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:154\u001b[0m, in \u001b[0;36mgenerate_artifacts\u001b[1;34m(model, requires_grad, frozen_params, loss, optimizer, artifact_directory, **extra_options)\u001b[0m\n\u001b[0;32m    149\u001b[0m     custom_op_library \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(custom_op_library)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m onnxblock\u001b[38;5;241m.\u001b[39mbase(model), onnxblock\u001b[38;5;241m.\u001b[39mcustom_op_library(\n\u001b[0;32m    152\u001b[0m     custom_op_library\n\u001b[0;32m    153\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m custom_op_library \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[1;32m--> 154\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     training_model, eval_model \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mto_model_proto()\n\u001b[0;32m    156\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mparameters()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\onnxblock.py:191\u001b[0m, in \u001b[0;36mTrainingBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m args \u001b[38;5;241m=\u001b[39m [args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs after edit\u001b[39m\u001b[38;5;124m\"\u001b[39m, args)\n\u001b[1;32m--> 191\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mshape_inference\u001b[38;5;241m.\u001b[39minfer_shapes(accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    195\u001b[0m _graph_utils\u001b[38;5;241m.\u001b[39mregister_graph_outputs(model, output)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:124\u001b[0m, in \u001b[0;36mgenerate_artifacts.<locals>._TrainingBlock.build\u001b[1;34m(self, *inputs_to_loss)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss_output, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(extra_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_output_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_to_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\blocks.py:50\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding block: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 50\u001b[0m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnx\\checker.py:148\u001b[0m, in \u001b[0;36mcheck_model\u001b[1;34m(model, full_check, skip_opset_compatibility_check)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetsizeof(protobuf_string) \u001b[38;5;241m>\u001b[39m MAXIMUM_PROTOBUF:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis protobuf of onnx model is too large (>2GB). Call check_model with model path instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[1;32m--> 148\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotobuf_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_opset_compatibility_check\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInferenceError\u001b[0m: [ShapeInferenceError] (op_type:SoftmaxCrossEntropyLoss, node name: onnx::SoftmaxCrossEntropyLoss::10): labels typestr: Tind, has unsupported type: tensor(float)"
     ]
    }
   ],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "num_named_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_named_params += 1\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "# onnx_model = onnx.load(\"tinyllama.onnx\")\n",
    "onnx_model = onnx.load(\"distilgpt2.onnx\")\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    # loss=artifacts.LossType.CrossEntropyLoss,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dialogueText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          folder  dialogueID                      date         from  \\\n",
      "0             3  126125.tsv  2008-04-23T14:55:00.000Z    bad_image   \n",
      "1             3  126125.tsv  2008-04-23T14:56:00.000Z    bad_image   \n",
      "2             3  126125.tsv  2008-04-23T14:57:00.000Z    lordleemo   \n",
      "3             3   64545.tsv  2009-08-01T06:22:00.000Z     mechtech   \n",
      "4             3   64545.tsv  2009-08-01T06:22:00.000Z     mechtech   \n",
      "...         ...         ...                       ...          ...   \n",
      "1038319       3   51506.tsv  2012-01-31T10:56:00.000Z           DJ   \n",
      "1038320       3   51506.tsv  2012-01-31T10:56:00.000Z     aeon-ltd   \n",
      "1038321       3   99669.tsv  2008-11-16T20:11:00.000Z      KR-data   \n",
      "1038322       3   99669.tsv  2008-11-16T20:12:00.000Z  outbackwifi   \n",
      "1038323       3   99669.tsv  2008-11-16T20:13:00.000Z      KR-data   \n",
      "\n",
      "                  to                                               text  \n",
      "0                NaN  Hello folks, please help me a bit with the fol...  \n",
      "1                NaN  Did I choose a bad channel? I ask because you ...  \n",
      "2          bad_image  the second sentence is better english   and we...  \n",
      "3                NaN                                       Sock Puppe?t  \n",
      "4                NaN                                               WTF?  \n",
      "...              ...                                                ...  \n",
      "1038319          NaN                                         anyone on?  \n",
      "1038320           DJ                                                yes  \n",
      "1038321          NaN  can I get a pastebin of someones menu.lst with...  \n",
      "1038322      KR-data                       http://pastebin.com/fe921690  \n",
      "1038323  outbackwifi                                             thanks  \n",
      "\n",
      "[1038324 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_convo =[]\n",
    "convos = []\n",
    "current_user = \"\"\n",
    "current_responder = \"\"\n",
    "\n",
    "for ind in df.index:\n",
    "    if len(current_user) == 0 or str(df['from'][ind]) == current_user:\n",
    "        # first convo OR continuing current user\n",
    "        current_user = str(df['from'][ind])\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    elif str(df['from'][ind]) != current_user and str(df['from'][ind]) != current_responder and len(current_responder) != 0:\n",
    "        # new user & new convo\n",
    "        convos.append(current_convo)\n",
    "        current_convo = []\n",
    "        current_user = str(df['from'][ind])\n",
    "        current_responder = \"\"\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    elif str(df['from'][ind]) != current_user:\n",
    "        if str(df['from'][ind]) != current_responder:\n",
    "            current_responder = str(df['from'][ind])\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"ERROR: fall through -- you missed a case!\", df['from'][ind], current_user, current_responder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345692\n",
      "[{'role': 'user', 'content': \"is there any reason why my 'network manager' icon in my tray does not scale when the other ones get resized when i change the size of the panel?\"}, {'role': 'assistant', 'content': 'probably not a vector icon'}, {'role': 'user', 'content': \"the other icons aren't vector icons either.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(len(convos))\n",
    "print(convos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = transformers.AutoTokenizer.from_pretrained(\"Xenova/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345692/345692 [00:36<00:00, 9429.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = Dataset.from_dict({\"chat\": convos})\n",
    "templated_convos_2 = dataset_dict.map(lambda x: {\"formatted_chat\": llama_tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat': [{'content': \"Hello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'Did I choose a bad channel? I ask because you seem to be dumb like windows user',\n",
       "   'role': 'user'},\n",
       "  {'content': 'the second sentence is better english   and we are not dumb',\n",
       "   'role': 'assistant'}],\n",
       " 'formatted_chat': \"Hello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?<|endoftext|>Did I choose a bad channel? I ask because you seem to be dumb like windows user<|endoftext|>the second sentence is better english   and we are not dumb<|endoftext|>\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_convos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat': [{'content': \"Hello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'Did I choose a bad channel? I ask because you seem to be dumb like windows user',\n",
       "   'role': 'user'},\n",
       "  {'content': 'the second sentence is better english   and we are not dumb',\n",
       "   'role': 'assistant'}],\n",
       " 'formatted_chat': \"<|user|>\\nHello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?</s>\\n<|user|>\\nDid I choose a bad channel? I ask because you seem to be dumb like windows user</s>\\n<|assistant|>\\nthe second sentence is better english   and we are not dumb</s>\\n\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_convos_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"formatted_chat_500_convos.txt\", \"w\")\n",
    "\n",
    "data_file.writelines(templated_convos_2[\"formatted_chat\"])\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training artifacts by using the Python training API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test generated artifacts\n",
    "# create checkpoint state\n",
    "\n",
    "state = ort_api.CheckpointState.load_checkpoint(\"checkpoint\")\n",
    "\n",
    "model = ort_api.Module('training_model.onnx', state, 'eval_model.onnx')\n",
    "\n",
    "optimizer = ort_api.Optimizer('optimizer_model.onnx', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  41%|████      | 140528/345692 [01:04<01:31, 2249.50 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map:  57%|█████▋    | 195501/345692 [01:29<01:04, 2330.82 examples/s]"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = templated_convos_2.map(lambda x: tokenizer(x[\"formatted_chat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\u001b[43mtokenized_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        logits, loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        optimizer.step()\n",
    "        model.lazy_reset_grad()\n",
    "        losses.append(loss.item())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
