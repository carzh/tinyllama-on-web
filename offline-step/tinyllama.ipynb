{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import torch\n",
    "import onnx\n",
    "import transformers\n",
    "# from fastT5 import export_and_get_onnx_model\n",
    "import onnxruntime.training.api as ort_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = export_and_get_onnx_model(\"MBZUAI/LaMini-T5-61M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"distilbert/distilgpt2\",\n",
    "# )\n",
    "\n",
    "# transformers_model = transformers.AutoModelForCausalLM.from_pretrained(\"MBZUAI/LaMini-Cerebras-111M\")\n",
    "# transformers_model = transformers.AutoModel.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "# transformers_model = transformers.AutoModel.from_pretrained(\"distilbert/distilgpt2\")\n",
    "transformers_model_gpt = transformers.GPT2Model.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 464, 3139,  286, 4881,  318, 6342,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "# tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "inputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "# model = FlatModel(pipeline.model)\n",
    "model = FlatModel(transformers_model_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:801: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n"
     ]
    }
   ],
   "source": [
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "                  \"distilgpt2.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING,\n",
    "                  do_constant_folding=False,\n",
    "                  opset_version=15,\n",
    "                  dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "num_named_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_named_params += 1\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "# onnx_model = onnx.load(\"tinyllama.onnx\")\n",
    "onnx_model = onnx.load(\"distilgpt2.onnx\")\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    # loss=artifacts.LossType.CrossEntropyLoss,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dialogueText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_convo =[]\n",
    "convos = []\n",
    "current_user = \"\"\n",
    "current_responder = \"\"\n",
    "\n",
    "for ind in df.index:\n",
    "    if len(current_user) == 0 or str(df['from'][ind]) == current_user:\n",
    "        # first convo OR continuing current user\n",
    "        current_user = str(df['from'][ind])\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    elif str(df['from'][ind]) != current_user and str(df['from'][ind]) != current_responder and len(current_responder) != 0:\n",
    "        # new user & new convo\n",
    "        convos.append(current_convo)\n",
    "        current_convo = []\n",
    "        current_user = str(df['from'][ind])\n",
    "        current_responder = \"\"\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    elif str(df['from'][ind]) != current_user:\n",
    "        if str(df['from'][ind]) != current_responder:\n",
    "            current_responder = str(df['from'][ind])\n",
    "        current_convo.append(\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": str(df['text'][ind])\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"ERROR: fall through -- you missed a case!\", df['from'][ind], current_user, current_responder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = transformers.AutoTokenizer.from_pretrained(\"Xenova/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345692/345692 [00:36<00:00, 9396.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = Dataset.from_dict({\"chat\": convos})\n",
    "templated_convos_2 = dataset_dict.map(lambda x: {\"formatted_chat\": llama_tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"formatted_chat_500_convos.txt\", \"w\")\n",
    "\n",
    "data_file.writelines(templated_convos_2[\"formatted_chat\"])\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training artifacts by using the Python training API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test generated artifacts\n",
    "# create checkpoint state\n",
    "\n",
    "state = ort_api.CheckpointState.load_checkpoint(\"checkpoint\")\n",
    "\n",
    "training_model = ort_api.Module('training_model.onnx', state, 'eval_model.onnx')\n",
    "\n",
    "optimizer = ort_api.Optimizer('optimizer_model.onnx', training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 345692/345692 [03:46<00:00, 1525.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = templated_convos_2.map(lambda x: tokenizer(x[\"formatted_chat\"], max_length=1024, padding='max_length', truncation=True, return_tensors='np'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['chat', 'formatted_chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1024)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(np.transpose(np.array(batch[\"input_ids\"][0])).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    training_model.train()\n",
    "    losses = []\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        print(i, 'out of ', len(dataloader))\n",
    "        input_ids = np.transpose(np.array(batch[\"input_ids\"][0]))\n",
    "        # input_ids = np.array(batch[\"input_ids\"][0])\n",
    "        # attention_mask = np.array(batch[\"attention_mask\"][0])\n",
    "        attention_mask = np.transpose(np.array(batch[\"attention_mask\"][0]))\n",
    "        print('input ids shape', input_ids.shape)\n",
    "        print('attention_mask shape', attention_mask.shape)\n",
    "        forward_inputs = [input_ids, attention_mask]\n",
    "        loss, _ = training_model(*forward_inputs)\n",
    "        print('after training')\n",
    "        optimizer.step()\n",
    "        print('after optimizing')\n",
    "        training_model.lazy_reset_grad()\n",
    "        losses.append(loss.item())\n",
    "        print(loss)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of  43212\n",
      "input ids shape (8, 1024)\n",
      "attention_mask shape (8, 1024)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:538 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'/model/Reshape_2_Grad/Reshape_1' Status Message: C:\\a\\_work\\1\\s\\onnxruntime\\core\\providers\\cpu\\tensor\\reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{}, requested shape:{8,1024,768}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask shape\u001b[39m\u001b[38;5;124m'\u001b[39m, attention_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     13\u001b[0m forward_inputs \u001b[38;5;241m=\u001b[39m [input_ids, attention_mask]\n\u001b[1;32m---> 14\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:113\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *user_inputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(OrtValue(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m fetches)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_np_input(user_inputs):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_generic_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43muser_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_step_with_ortvalues(user_inputs)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-new\\lib\\site-packages\\onnxruntime\\training\\api\\module.py:85\u001b[0m, in \u001b[0;36mModule.__call__.<locals>._take_generic_step\u001b[1;34m(forward_inputs)\u001b[0m\n\u001b[0;32m     83\u001b[0m fetches \u001b[38;5;241m=\u001b[39m OrtValueVector()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval_step(forward_inputs, fetches)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\training_api\\module.cc:538 onnxruntime::training::api::Module::TrainStep [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'/model/Reshape_2_Grad/Reshape_1' Status Message: C:\\a\\_work\\1\\s\\onnxruntime\\core\\providers\\cpu\\tensor\\reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{}, requested shape:{8,1024,768}\n\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
