{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import torch\n",
    "import onnx\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "inputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "model = FlatModel(pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\torch\\onnx\\utils.py:823: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:343: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.all(attention_mask == 1):\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:161: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:381: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:244: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if left_masked_rows.shape[0] == 0:\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:150: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:714: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n"
     ]
    }
   ],
   "source": [
    "input_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "                  \"tinyllama.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside artifacts.py generate_artifacts\n",
      "{}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This protobuf of onnx model is too large (>2GB). Call check_model with model path instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     frozen_params\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[0;32m     12\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinyllama.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozen_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:156\u001b[0m, in \u001b[0;36mgenerate_artifacts\u001b[1;34m(model, requires_grad, frozen_params, loss, optimizer, artifact_directory, **extra_options)\u001b[0m\n\u001b[0;32m    151\u001b[0m     custom_op_library \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(custom_op_library)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m onnxblock\u001b[38;5;241m.\u001b[39mbase(model), onnxblock\u001b[38;5;241m.\u001b[39mcustom_op_library(\n\u001b[0;32m    154\u001b[0m     custom_op_library\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m custom_op_library \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[1;32m--> 156\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     training_model, eval_model \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mto_model_proto()\n\u001b[0;32m    158\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mparameters()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\onnxblock.py:188\u001b[0m, in \u001b[0;36mTrainingBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    186\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding training block \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    190\u001b[0m model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mshape_inference\u001b[38;5;241m.\u001b[39minfer_shapes(accessor\u001b[38;5;241m.\u001b[39m_GLOBAL_ACCESSOR\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    192\u001b[0m _graph_utils\u001b[38;5;241m.\u001b[39mregister_graph_outputs(model, output)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:126\u001b[0m, in \u001b[0;36mgenerate_artifacts.<locals>._TrainingBlock.build\u001b[1;34m(self, *inputs_to_loss)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss_output, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(extra_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_output_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_to_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\blocks.py:50\u001b[0m, in \u001b[0;36mBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding block: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 50\u001b[0m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts\\lib\\site-packages\\onnx\\checker.py:145\u001b[0m, in \u001b[0;36mcheck_model\u001b[1;34m(model, full_check, skip_opset_compatibility_check)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# If the protobuf is larger than 2GB,\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# remind users should use the model path to check\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetsizeof(protobuf_string) \u001b[38;5;241m>\u001b[39m MAXIMUM_PROTOBUF:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis protobuf of onnx model is too large (>2GB). Call check_model with model path instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m C\u001b[38;5;241m.\u001b[39mcheck_model(protobuf_string, full_check, skip_opset_compatibility_check)\n",
      "\u001b[1;31mValueError\u001b[0m: This protobuf of onnx model is too large (>2GB). Call check_model with model path instead."
     ]
    }
   ],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "onnx_model = onnx.load(\"tinyllama.onnx\")\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"position_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
