{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.training import artifacts\n",
    "import torch\n",
    "import onnx\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([32000, 2048]) in the checkpoint and torch.Size([32000, 4096]) in the model instantiated\n",
      "- model.embed_tokens.weight: found shape torch.Size([32000, 2048]) in the checkpoint and torch.Size([32000, 4096]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([2048]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.0.mlp.down_proj.weight: found shape torch.Size([2048, 5632]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.0.mlp.gate_proj.weight: found shape torch.Size([5632, 2048]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.0.mlp.up_proj.weight: found shape torch.Size([5632, 2048]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([2048]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([256, 2048]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.o_proj.weight: found shape torch.Size([2048, 2048]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.q_proj.weight: found shape torch.Size([2048, 2048]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([256, 2048]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([2048]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")\n",
    "\n",
    "transformers_model = transformers.LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", config=transformers.LlamaConfig(num_hidden_layers=1), ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "inputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *local_inputs):\n",
    "        return self.model(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "# model = FlatModel(pipeline.model)\n",
    "model = FlatModel(transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1057: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_length > self.causal_mask.shape[-1]:\n"
     ]
    }
   ],
   "source": [
    "input_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "output_names = [\"loss\", \"logits\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "                  \"tinyllama.onnx\",\n",
    "                  input_names = input_names, \n",
    "                  output_names = output_names,\n",
    "                  export_params=True,\n",
    "                  training=torch.onnx.TrainingMode.TRAINING,\n",
    "                  do_constant_folding=False,\n",
    "                  dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hello\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "C:\\a\\_work\\1\\s\\orttraining\\orttraining\\python\\orttraining_pybind_state.cc:617 onnxruntime::python::addObjectMethodsForTraining::<lambda_bb073955579d98a9752b4412a7cc4c49>::operator () [ONNXRuntimeError] : 1 : FAIL : Node (/model/lm_head/MatMul_Grad/Reshape_4) Op (Reshape) [ShapeInferenceError] Dimension could not be inferred: incompatible shapes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m requires_grad \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m onnx_model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minitializer]\n\u001b[0;32m     15\u001b[0m frozen_params \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 17\u001b[0m \u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrozen_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozen_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_input_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\artifacts.py:175\u001b[0m, in \u001b[0;36mgenerate_artifacts\u001b[1;34m(model, requires_grad, frozen_params, loss, optimizer, artifact_directory, prefix, ort_format, custom_op_library, additional_output_names, nominal_checkpoint, loss_input_names)\u001b[0m\n\u001b[0;32m    168\u001b[0m     custom_op_library_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(custom_op_library)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m onnxblock\u001b[38;5;241m.\u001b[39mbase(model), (\n\u001b[0;32m    171\u001b[0m     onnxblock\u001b[38;5;241m.\u001b[39mcustom_op_library(custom_op_library_path)\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m custom_op_library \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    174\u001b[0m ):\n\u001b[1;32m--> 175\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     training_model, eval_model \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mto_model_proto()\n\u001b[0;32m    177\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m training_block\u001b[38;5;241m.\u001b[39mparameters()\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\onnxblock.py:204\u001b[0m, in \u001b[0;36mTrainingBlock.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m _training_graph_utils\u001b[38;5;241m.\u001b[39mget_model_parameters(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frozen_params)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Build the gradient graph. The gradient graph building is composed of the following steps:\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m#   - Move all model parameters to model inputs.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m#   - Run orttraining graph transformers on the model.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m#   - Add the gradient graph to the optimized model.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# The order of model inputs after gradient graph building is: user inputs, model parameters as inputs\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# The order of the model outputs is: user outputs, model parameter gradients (in the order of parameter inputs)\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_model \u001b[38;5;241m=\u001b[39m \u001b[43m_training_graph_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_gradient_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_frozen_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GLOBAL_CUSTOM_OP_LIBRARY\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding gradient accumulation nodes for training block \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    210\u001b[0m _training_graph_utils\u001b[38;5;241m.\u001b[39mbuild_gradient_accumulation_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_grad)\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\_training_graph_utils.py:130\u001b[0m, in \u001b[0;36mbuild_gradient_graph\u001b[1;34m(model, requires_grad, frozen_params, output_names, custom_op_library)\u001b[0m\n\u001b[0;32m    127\u001b[0m optimized_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload_from_string(get_optimized_model(model\u001b[38;5;241m.\u001b[39mSerializeToString(), requires_grad, options))\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Assumption is that the first graph output is the loss output\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m gradient_model \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_model_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m _reorder_outputs(gradient_model, output_names, requires_grad)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient_model, eval_model\n",
      "File \u001b[1;32mc:\\Users\\carolinezhu\\AppData\\Local\\anaconda3\\envs\\generate-artifacts-3\\lib\\site-packages\\onnxruntime\\training\\onnxblock\\_training_graph_utils.py:84\u001b[0m, in \u001b[0;36m_gradient_model_for\u001b[1;34m(model, requires_grad, loss_name, options)\u001b[0m\n\u001b[0;32m     79\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loss output is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. The gradient graph will be built starting from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_name, loss_name\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m builder \u001b[38;5;241m=\u001b[39m GradientGraphBuilder(model\u001b[38;5;241m.\u001b[39mSerializeToString(), {loss_name}, requires_grad, loss_name, options)\n\u001b[1;32m---> 84\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m onnx\u001b[38;5;241m.\u001b[39mload_from_string(builder\u001b[38;5;241m.\u001b[39mget_model())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: C:\\a\\_work\\1\\s\\orttraining\\orttraining\\python\\orttraining_pybind_state.cc:617 onnxruntime::python::addObjectMethodsForTraining::<lambda_bb073955579d98a9752b4412a7cc4c49>::operator () [ONNXRuntimeError] : 1 : FAIL : Node (/model/lm_head/MatMul_Grad/Reshape_4) Op (Reshape) [ShapeInferenceError] Dimension could not be inferred: incompatible shapes\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = []\n",
    "# frozen_params = []\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         requires_grad.append(name)\n",
    "#     else:\n",
    "#         frozen_params.append(name)\n",
    "\n",
    "# for name, param in model.named_buffers():\n",
    "#     frozen_params.append(name)\n",
    "\n",
    "onnx_model = onnx.load(\"tinyllama.onnx\")\n",
    "\n",
    "requires_grad = [param.name for param in onnx_model.graph.initializer]\n",
    "frozen_params = []\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss_input_names=[\"loss\"]\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
